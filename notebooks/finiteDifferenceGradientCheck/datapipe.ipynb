{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a5c8da6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from transformer import DataPipe, Attention, Attention_init\n",
    "from llm_architect import gradCheckTransformer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3bd05844",
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = np.random.default_rng(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "46757584",
   "metadata": {},
   "outputs": [],
   "source": [
    "B, T, C = 2, 4, 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6be7d1f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "attention_init = Attention_init.randomInitial(1000, B, T, C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "beabaf4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "attention1 = Attention(attention_init, name=\"attention1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5d53cf15",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = rng.random((B, T, C), dtype = np.float64)\n",
    "G = rng.random((B, T, C), dtype = np.float64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "42ae5407",
   "metadata": {},
   "outputs": [],
   "source": [
    "datapipe1 = DataPipe(attention1, \"pipe1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bb7ae9bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[1.04277148, 1.20768369, 0.94188359, 0.40649589, 1.05751805,\n",
       "         0.38207256],\n",
       "        [0.5630912 , 1.5073631 , 1.10334355, 1.72744415, 1.61074444,\n",
       "         0.49674533],\n",
       "        [0.37971482, 1.96799116, 1.33999433, 0.56076566, 0.40782647,\n",
       "         1.25012937],\n",
       "        [1.30520863, 1.79761506, 1.94952756, 0.30786474, 1.39817855,\n",
       "         0.89448289]],\n",
       "\n",
       "       [[0.03502642, 0.58204981, 0.76247322, 0.64205582, 1.88508934,\n",
       "         1.40533395],\n",
       "        [0.27290064, 0.68641815, 1.62398921, 0.296988  , 0.11865138,\n",
       "         0.62883327],\n",
       "        [0.84031291, 1.61603542, 0.01901517, 0.90816758, 1.11737397,\n",
       "         0.00577727],\n",
       "        [0.59551514, 0.10759822, 1.13533751, 1.8811163 , 1.44854743,\n",
       "         1.71275618]]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datapipe1.forward(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "035eb6a7",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Attention' object has no attribute 'attentions'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mdatapipe1\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mG\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mD:\\python\\llm-scratch\\src\\transformer\\dataPipe\\DataPipe.py:22\u001b[39m, in \u001b[36mDataPipe.backward\u001b[39m\u001b[34m(self, gradientOfOutput)\u001b[39m\n\u001b[32m     21\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mbackward\u001b[39m(\u001b[38;5;28mself\u001b[39m, gradientOfOutput):\n\u001b[32m---> \u001b[39m\u001b[32m22\u001b[39m     \u001b[38;5;28mself\u001b[39m.gradientOfX_sideway = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbranchObject\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgradientOfOutput\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     23\u001b[39m     \u001b[38;5;28mself\u001b[39m.gradientOfInputPipe = gradientOfOutput + \u001b[38;5;28mself\u001b[39m.gradientOfX_sideway\n\u001b[32m     24\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.gradientOfInputPipe\n",
      "\u001b[36mFile \u001b[39m\u001b[32mD:\\python\\llm-scratch\\src\\transformer\\attention\\Attention.py:149\u001b[39m, in \u001b[36mAttention.backward\u001b[39m\u001b[34m(self, gradientOfOutput)\u001b[39m\n\u001b[32m    146\u001b[39m d_merge_output = gradientOfOutput\n\u001b[32m    147\u001b[39m d_output = \u001b[38;5;28mself\u001b[39m.split_heads(d_merge_output, attentionHead=\u001b[38;5;28mself\u001b[39m.H)\n\u001b[32m--> \u001b[39m\u001b[32m149\u001b[39m d_V_split = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mattentions\u001b[49m.transpose(\u001b[32m0\u001b[39m, \u001b[32m1\u001b[39m, \u001b[32m3\u001b[39m, \u001b[32m2\u001b[39m) @ d_output\n\u001b[32m    150\u001b[39m d_attentions = d_output @ \u001b[38;5;28mself\u001b[39m.V_split.transpose(\u001b[32m0\u001b[39m, \u001b[32m1\u001b[39m, \u001b[32m3\u001b[39m, \u001b[32m2\u001b[39m)\n\u001b[32m    152\u001b[39m d_scores = softmaxBackward(d_attentions, \u001b[38;5;28mself\u001b[39m.softmax_cache)\n",
      "\u001b[31mAttributeError\u001b[39m: 'Attention' object has no attribute 'attentions'"
     ]
    }
   ],
   "source": [
    "datapipe1.backward(G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "03e6b811",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Attention' object has no attribute 'attentions'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m attention1.forward(X)\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[43mattention1\u001b[49m\u001b[43m.\u001b[49m\u001b[43mattentions\u001b[49m\n",
      "\u001b[31mAttributeError\u001b[39m: 'Attention' object has no attribute 'attentions'"
     ]
    }
   ],
   "source": [
    "attention1.forward(X)\n",
    "attention1.attentions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "991c67c3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aea7c92a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (llm-scratch)",
   "language": "python",
   "name": "llm-scratch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
