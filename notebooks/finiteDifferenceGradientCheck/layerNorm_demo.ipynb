{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9aeec6c0",
   "metadata": {},
   "source": [
    "A **finite-diffence gradient check** (sometimes called a _numerical gradient check_) is quick way to verify that your **backward()** math is correct by comparing it to an approximate gradient computed from the forward function only."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0ddb0fe",
   "metadata": {},
   "source": [
    "### Idea"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e89d0466",
   "metadata": {},
   "source": [
    "if you slightly nudge one input value $x_i$ up and down by a tiny $\\epsilon$, you can estimate:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd071ecd",
   "metadata": {},
   "source": [
    "$$\\frac{\\partial L}{\\partial x_i} \\approx \\frac{L\\left( x_i + \\epsilon \\right) - L\\left(x_i - \\epsilon\\right)} {2 \\epsilon}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d31af0e7",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "Then you compare this \"numerical\" gradient with your backprops result."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d71d84d",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "### Minimal example for LayerNorm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91fe7df9",
   "metadata": {},
   "source": [
    "We'll create a fake loss:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "511c5805",
   "metadata": {},
   "source": [
    "$$ \\mathcal{L} = \\sum\\left(layer\\_norm(X) \\odot G\\right)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af9f645f",
   "metadata": {},
   "source": [
    "\n",
    "where $G$ is a random upstream gradient (same shape as output).\n",
    "This makes the analytical gradient equal to\n",
    "```\n",
    "layer_norm_backward(G, cache)\n",
    "```\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
