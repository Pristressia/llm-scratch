{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9aeec6c0",
   "metadata": {},
   "source": [
    "A **finite-diffence gradient check** (sometimes called a _numerical gradient check_) is quick way to verify that your **backward()** math is correct by comparing it to an approximate gradient computed from the forward function only."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0ddb0fe",
   "metadata": {},
   "source": [
    "### Idea"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e89d0466",
   "metadata": {},
   "source": [
    "if you slightly nudge one input value $x_i$ up and down by a tiny $\\epsilon$, you can estimate:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd071ecd",
   "metadata": {},
   "source": [
    "$$\\frac{\\partial L}{\\partial x_i} \\approx \\frac{L\\left( x_i + \\epsilon \\right) - L\\left(x_i - \\epsilon\\right)} {2 \\epsilon}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d31af0e7",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "Then you compare this \"numerical\" gradient with your backprops result."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d71d84d",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "### Minimal example for LayerNorm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91fe7df9",
   "metadata": {},
   "source": [
    "We'll create a fake loss:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "511c5805",
   "metadata": {},
   "source": [
    "$$ \\mathcal{L} = \\sum\\left(layer\\_norm(X) \\odot G\\right)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af9f645f",
   "metadata": {},
   "source": [
    "\n",
    "where $G$ is a random upstream gradient (same shape as output).\n",
    "This makes the analytical gradient equal to\n",
    "```Python\n",
    "layer_norm_backward(G, cache)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7af018be",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1829aa82",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llm_operator import layer_norm, layer_norm_backward, Layer_norm_cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91536761",
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = np.random.default_rng(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d2a7d656",
   "metadata": {},
   "outputs": [],
   "source": [
    "def grad_check_layer_norm(layer_norm, layer_norm_backward, eps=1e-6):\n",
    "    rng = np.random.default_rng(1000)\n",
    "\n",
    "    B, T, C = 2, 4, 6\n",
    "\n",
    "    X = -rng.random((B, T, C), dtype=np.float64)\n",
    "    G = -rng.random((B, T, C), dtype=np.float64) # upstream gradient\n",
    "\n",
    "    # forward\n",
    "    Y, cache = layer_norm(X, epsilon = 1e-5)\n",
    "\n",
    "    # analytical gradient\n",
    "    dX = layer_norm_backward(G, cache)\n",
    "\n",
    "    # numerical gradient\n",
    "    dX_num = np.zeros_like(X)\n",
    "    it = np.nditer(X, flags=['multi_index'], op_flags=['readwrite'])\n",
    "\n",
    "    while not it.finished:\n",
    "        idx = it.multi_index\n",
    "        old = X[idx]\n",
    "\n",
    "        X[idx] = old + eps\n",
    "        Yp, _ = layer_norm(X, epsilon=1e-5)\n",
    "        Lp = np.sum(Yp * G)\n",
    "\n",
    "        X[idx] = old - eps\n",
    "        Ym, _ = layer_norm(X, epsilon=1e-5)\n",
    "        Lm = np.sum(Ym * G)\n",
    "\n",
    "        X[idx] = old\n",
    "        dX_num[idx] = (Lp - Lm) / (2 * eps)\n",
    "\n",
    "        it.iternext()\n",
    "\n",
    "    # compare\n",
    "\n",
    "    max_abs = np.max(np.abs(dX - dX_num))\n",
    "    rel = max_abs / (np.max(np.abs(dX) + np.abs(dX_num)) + 1e-22)\n",
    "\n",
    "\n",
    "    print(\"max_abs_diff:\", max_abs)\n",
    "    print(\"relative_diff:\", rel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "99503bee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max_abs_diff: 1.1812836819835582e-09\n",
      "relative_diff: 2.61470216195465e-10\n"
     ]
    }
   ],
   "source": [
    "grad_check_layer_norm(layer_norm, layer_norm_backward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd8f30c5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm-scratch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
